{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Full-Model.ipynb","provenance":[{"file_id":"1rrcsLEJhhI82ldKUS6A059PeRv-nRaZB","timestamp":1632306949667}],"collapsed_sections":["OvNpXcZLVdyR"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"id":"jKeRe84YZTU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633762742783,"user_tz":-330,"elapsed":404,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"4bb16d47-5bd5-45f3-ad4d-60f07291438f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"JU4reX5xdttq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633762743453,"user_tz":-330,"elapsed":10,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"801daa72-d1ca-49c1-a4c7-291a8c0a613e"},"source":["%cd /content/drive/MyDrive/codes/EnergyForecastingCompetition"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/codes/EnergyForecastingCompetition\n"]}]},{"cell_type":"code","metadata":{"id":"qyRk2awOfZP7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633762744052,"user_tz":-330,"elapsed":605,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"db340e21-464f-4ddf-ac4e-4b085273262b"},"source":["%ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mcomp_files\u001b[0m/                          Forecasting.ipynb\n","'Copy of Model-Buildings.ipynb'       Model-Buildings.ipynb\n","'Copy of SolarPanel05-Minoli.ipynb'   Model.ipynb\n"," csvfile.csv                         'Model- Mudith.ipynb'\n"," csvfile.gsheet                      'Monthly Building Data.gdoc'\n"," data_loader.py                       phase_1_data.tsf\n"," data_loader.py.1                     \u001b[01;34m__pycache__\u001b[0m/\n"," data_loader.py.2                     SolarPanel05-Minoli.ipynb\n","'Dataset Analysis.gslides'            Stacking.ipynb\n"," ERA5_Weather_Data_Monash.csv         sub.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"NxS4xTSCPtJa"},"source":["## **Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"nQrx6COdPrBx"},"source":["from datetime import datetime\n","from numpy import distutils\n","from sklearn.model_selection import train_test_split\n","import csv\n","import distutils\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import lightgbm as lgbm\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HxxvRE3GMcCL"},"source":["## **Loading the Dataset**"]},{"cell_type":"code","metadata":{"id":"3H3LhCHBiNK0"},"source":["from datetime import datetime\n","from numpy import distutils\n","from sklearn.model_selection import train_test_split\n","import csv\n","import distutils\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","#from prophet import Prophet\n","\n","\n","# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n","#\n","# Parameters\n","# full_file_path_and_name - complete .tsf file path\n","# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n","# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n","def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with = 'NaN', value_column_name = \"series_value\"):\n","    col_names = []\n","    col_types = []\n","    all_data = {}\n","    line_count = 0\n","    frequency = None\n","    forecast_horizon = None\n","    contain_missing_values = None\n","    contain_equal_length = None\n","    found_data_tag = False\n","    found_data_section = False\n","    started_reading_data_section = False\n","\n","    with open(full_file_path_and_name, 'r', encoding='cp1252') as file:\n","        for line in file:\n","            # Strip white space from start/end of line\n","            line = line.strip()\n","\n","            if line:\n","                if line.startswith(\"@\"): # Read meta-data\n","                    if not line.startswith(\"@data\"):\n","                        line_content = line.split(\" \")\n","                        if line.startswith(\"@attribute\"):\n","                            if (len(line_content) != 3):  # Attributes have both name and type\n","                                raise Exception(\"Invalid meta-data specification.\")\n","\n","                            col_names.append(line_content[1])\n","                            col_types.append(line_content[2])\n","                        else:\n","                            if len(line_content) != 2:  # Other meta-data have only values\n","                                raise Exception(\"Invalid meta-data specification.\")\n","\n","                    \n","                            if line.startswith(\"@frequency\"):\n","                                frequency = line_content[1]\n","                            elif line.startswith(\"@horizon\"):\n","                                forecast_horizon = int(line_content[1])\n","                            elif line.startswith(\"@missing\"):\n","                                contain_missing_values = bool(distutils.util.strtobool(line_content[1]))\n","                            elif line.startswith(\"@equallength\"):\n","                                contain_equal_length = bool(distutils.util.strtobool(line_content[1]))\n","\n","                    else:\n","                        if len(col_names) == 0:\n","                            raise Exception(\"Missing attribute section. Attribute section must come before data.\")\n","\n","                        found_data_tag = True\n","                elif not line.startswith(\"#\"):\n","                    if len(col_names) == 0:\n","                        raise Exception(\"Missing attribute section. Attribute section must come before data.\")\n","                    elif not found_data_tag:\n","                        raise Exception(\"Missing @data tag.\")\n","                    else:\n","                        if not started_reading_data_section:\n","                            started_reading_data_section = True\n","                            found_data_section = True\n","                            all_series = []\n","\n","                            for col in col_names:\n","                                all_data[col] = []\n","\n","                        full_info = line.split(\":\")\n","\n","                        if len(full_info) != (len(col_names) + 1):\n","                            raise Exception(\"Missing attributes/values in series.\")\n","\n","                        series = full_info[len(full_info) - 1]\n","                        series = series.split(\",\")\n","\n","                        if(len(series) == 0):\n","                            raise Exception(\"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\")\n","\n","                        numeric_series = []\n","\n","                        for val in series:\n","                            if val == \"?\":\n","                                numeric_series.append(replace_missing_vals_with)\n","                            else:\n","                                numeric_series.append(float(val))\n","\n","                        if (numeric_series.count(replace_missing_vals_with) == len(numeric_series)):\n","                            raise Exception(\"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\")\n","\n","                        all_series.append(pd.Series(numeric_series).array)\n","\n","                        for i in range(len(col_names)):\n","                            att_val = None\n","                            if col_types[i] == \"numeric\":\n","                                att_val = int(full_info[i])\n","                            elif col_types[i] == \"string\":\n","                                att_val = str(full_info[i])\n","                            elif col_types[i] == \"date\":\n","                                att_val = datetime.strptime(full_info[i], '%Y-%m-%d %H-%M-%S')\n","                            else:\n","                                raise Exception(\"Invalid attribute type.\") # Currently, the code supports only numeric, string and date types. Extend this as required.\n","\n","                            if(att_val == None):\n","                                raise Exception(\"Invalid attribute value.\")\n","                            else:\n","                                all_data[col_names[i]].append(att_val)\n","\n","                line_count = line_count + 1\n","\n","        if line_count == 0:\n","            raise Exception(\"Empty file.\")\n","        if len(col_names) == 0:\n","            raise Exception(\"Missing attribute section.\")\n","        if not found_data_section:\n","            raise Exception(\"Missing series information under data section.\")\n","\n","        all_data[value_column_name] = all_series\n","        \n","        loaded_data = pd.DataFrame(all_data)\n","        #print(loaded_data.iloc[0,2])\n","        return loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"id_bM7g1k9pM"},"source":["loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"phase_1_data.tsf\",replace_missing_vals_with=np.nan)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1_50Eg528kM"},"source":["def getTimeBin(curr_time):#return the time index value of a day, when given the time of timestamp\n","  return curr_time.hour*4 + curr_time.minute//15"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-ldxFF41x5-"},"source":["**Adding Features to the Input**"]},{"cell_type":"code","metadata":{"id":"ASFQGboT9VNX"},"source":["def addNoiseToFeatures(features,noOfNoises):\n","  f_N= len(features)\n","  #add to noises\n","  for i in range(noOfNoises):\n","    noisePos= random.randint(0,f_N-1)\n","    features[noisePos]=random.uniform(0.0,2)*features[noisePos]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZdoH8kDLbTS"},"source":["## **Build the light GBM Model**\n","\n","Setting the parameters\n"," https://lightgbm.readthedocs.io/en/latest/Parameters.html#parameters-format"]},{"cell_type":"code","metadata":{"id":"aP15_wxjApHx"},"source":["\n","params = {\n","        'nthread': 10,\n","         'max_depth': 5,\n","#         'max_depth': 9,\n","        'task': 'train',\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression_l1',\n","        'metric': 'mape', # this is abs(a-e)/max(1,a)\n","#         'num_leaves': 39,\n","        'num_leaves': 64,\n","        'learning_rate': 0.075,\n","       'feature_fraction': 0.9,\n","#         'feature_fraction': 0.8108472661400657,\n","#         'bagging_fraction': 0.9837558288375402,\n","       'bagging_fraction': 0.8,\n","        'bagging_freq': 5,\n","        'lambda_l1': 3.097758978478437,\n","        'lambda_l2': 2.9482537987198496,\n","#       'lambda_l1': 0.06,\n","#       'lambda_l2': 0.1,\n","        'verbose': 1,\n","        'min_child_weight': 6.996211413900573,\n","        'min_split_gain': 0.037310344962162616,\n"," }\n","# \"\"\"\n","# params = {\n","#           \"objective\" : \"poisson\",\n","#           \"metric\" :\"rmse\",\n","#           \"force_row_wise\" : True,\n","#           \"learning_rate\" : 0.075,\n","#           \"sub_row\" : 0.75,\n","#           \"bagging_freq\" : 1,\n","#           \"lambda_l2\" : 0.1,\n","#           \"metric\": [\"rmse\"],\n","#           'verbosity': 1,\n","#           'num_iterations' : 1200,\n","#           'num_leaves': 128,\n","#           \"min_data_in_leaf\": 100,\n","#          }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8hYUpuwOHwi"},"source":["**Defining the model**\n","\n","https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html#lightgbm.train\n","\n","lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, fobj=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto', early_stopping_rounds=None, evals_result=None, verbose_eval='warn', learning_rates=None, keep_training_booster=False, callbacks=None)\n"]},{"cell_type":"code","metadata":{"id":"PdNed1ZmggLb"},"source":["def createDatasets(building_number, lag, start_index=0, number_of_days_to_predict=31):\n","\n","  # Extract Data\n","  building_series = loaded_data.iloc[building_number]['series_value'][start_index:].to_numpy()\n","  \n","  times = pd.date_range(loaded_data.iloc[building_number]['start_timestamp'], periods=len(loaded_data.iloc[building_number]['series_value']), freq='15min')\n","  times=times[start_index:]\n","\n","  # Treating null values\n","  count=0\n","  for i,bv in enumerate(building_series):\n","    if(pd.isna(bv)): \n","      count+=1\n","      building_series[i]=building_series[i-96] # values of previous day\n","\n","  train_data_x = []\n","  train_data_y = []\n","  \n","\n","  for i in range(building_series.shape[0]-lag):\n","    features = list(building_series[i:i+lag])\n","    \n","    window_mean = np.mean(building_series[i:i+lag])\n","    window_std = np.std(building_series[i:i+lag])\n","\n","  # features.append(getTimeBin(times[i+lag]))\n","  # features.append(times[i+lag].hour)\n","  # features.append(times[i+lag].dayofweek)\n","  # features.append(times[i+lag].month)\n","  # features.append(window_mean)\n","  # features.append(window_std)\n","\n","    month=times[i+lag].month\n","\n","    train_data_x.append(np.array(features))\n","    train_data_y.append(building_series[i+lag])\n","\n","  X = np.array(train_data_x)\n","  y = np.array(train_data_y)\n","\n","  X_normed=X/X.mean(axis=0)\n","  y_normed=y/y.mean(axis=0)\n","  y_mn=y.mean(axis=0)\n","\n","  # print(y_mn)\n","  # print(X_normed.shape)\n","  # print(y_normed.shape)\n","\n","  X_train, X_val, y_train, y_val = train_test_split(X_normed, y_normed, test_size=0.2, shuffle=True) # 0.25 x 0.8 = 0.2\n","\n","  lgb_training_data = lgbm.Dataset(X_train,label=y_train) \n","  lgb_valid_data = lgbm.Dataset(X_val,label=y_val,reference=lgb_training_data)\n","\n","  return lgb_training_data, lgb_valid_data, train_data_x, train_data_y, X_normed, y_normed, y_mn, times\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMlG4ZRQqX5C"},"source":["def createSolarDatasets(raw_number, lag, start_index=0, number_of_days_to_predict=31):\n","  solar_panel = loaded_data.iloc[raw_number]['series_value'].to_numpy()\n","  times = pd.date_range(loaded_data.iloc[raw_number]['start_timestamp'], periods=len(loaded_data.iloc[raw_number]['series_value']), freq='15min')\n","  weather_df = pd.read_csv(\"ERA5_Weather_Data_Monash.csv\")\n","  #weather_df = weather_df[82622:]   #2019-06-05 14:00:00 onwards\n","\n","  sr = []\n","  tp = []\n","  cc = []\n","\n","  #weather_times= pd.date_range('2010-01-01 00:00:00', '2021-10-01 00:00:00', freq='1H')\n","\n","  #weather_start_index=\n","  for i in range(82622,100057):\n","    for _ in range(4):   #repeat 4 times for the 15 minute intervals in 1 hour\n","      sr.append(weather_df['surface_solar_radiation (W/m^2)'][i])\n","      tp.append(weather_df['temperature (degC)'][i])\n","      cc.append(weather_df['total_cloud_cover (0-1)'][i])\n","\n","  sr1 = sr\n","  tp1 = tp\n","  cc1 = cc\n","\n","  solar_panel_norm = solar_panel\n","\n","   # Treating null values\n","  count=0\n","  for i,bv in enumerate(solar_panel):\n","    if(pd.isna(bv)): \n","      count+=1\n","      solar_panel[i]=solar_panel[i-96] # values of previous day\n","\n","  train_data_x = []\n","  train_data_y = []\n","  # lag = 10\n","\n","  for i in range(solar_panel.shape[0]-lag):\n","    features = list(solar_panel[i:i+lag])\n","    \n","    window_mean = np.mean(solar_panel[i:i+lag])\n","    window_std = np.std(solar_panel[i:i+lag])\n","\n","    features.append(times[i+lag].hour//24)\n","    features.append(times[i+lag].month//24)\n","    features.append(window_mean)\n","    features.append(window_std)\n","\n","    features.append(sr1[i+lag])  #get mean of all solar radiation data in the range\n","    features.append(tp1[i+lag])  #tempaeratures\n","    features.append(cc1[i+lag])  #cloud cover\n","    \n","  # if(i<10): print(tmp)\n","    train_data_x.append(np.array(features))\n","    train_data_y.append(solar_panel[i+lag])\n","\n","  X = np.array(train_data_x)\n","  y = np.array(train_data_y)\n","\n","  X_normed=X/X.mean(axis=0)\n","  y_normed=y/y.mean(axis=0)\n","  y_mn=y.mean(axis=0)\n","\n","  # print(y_mn)\n","  # print(X_normed.shape)\n","  # print(y_normed.shape)\n","\n","  X_train, X_val, y_train, y_val = train_test_split(X_normed, y_normed, test_size=0.2, shuffle=True) # 0.25 x 0.8 = 0.2\n","\n","  lgb_training_data = lgbm.Dataset(X_train,label=y_train) \n","  lgb_valid_data = lgbm.Dataset(X_val,label=y_val,reference=lgb_training_data)\n","\n","  return lgb_training_data, lgb_valid_data, train_data_x, train_data_y, X_normed, y_normed, y_mn, times"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhvBWnXNRfVi"},"source":["## Predicting a Month Ahead"]},{"cell_type":"code","metadata":{"id":"L5uJIDEBuJce"},"source":["def calculatePredictions(building_number, start_index=0, number_of_days_to_predict=31, start_date_str='2020-09-01'):\n","  lag = 96*30\n","  \n","  # lgb_training_data, lgb_valid_data, train_data_x, train_data_y, X_normed, y_normed, y_mn, times = createDatasets(building_number, lag, start_index=0, number_of_days_to_predict=31)\n","  lgb_training_data, lgb_valid_data, train_data_x, train_data_y, X_normed, y_normed, y_mn, times = createSolarDatasets(building_number, lag, start_index=0, number_of_days_to_predict=31)\n","\n","  # print(train_data_x)\n","\n","  model = lgbm.train(params,\n","                       lgb_training_data,\n","                       valid_sets=lgb_valid_data,\n","                       num_boost_round=2000, #5000\n","                       early_stopping_rounds=100 ,\n","                       verbose_eval=50)\n","  \n","  pred_time_count=4*24*number_of_days_to_predict\n","\n","  #month_predictions\n","  pred_times=pd.date_range(start_date_str, periods=pred_time_count, freq='15min')\n","\n","\n","  pred_y=[]\n","  pred_y.append(model.predict([train_data_x[-1]])[-1])\n","\n","  last_lag_y=list(y_normed[-lag+1:])\n","\n","  last_lag_y.append(pred_y[-1]) # x for first day in month, pred_y[0] is model output\n","\n","  for i in range(1,pred_time_count):\n","\n","    features = list(last_lag_y)\n","\n","    window_mean = np.mean(features)\n","    window_std = np.std(features)\n","\n","  # features.append(getTimeBin(pred_times[i]))\n","    # features.append(pred_times[i].hour)\n","    # features.append(pred_times[i].dayofweek)\n","    # features.append(9)#pred_times[i].month)\n","    # features.append(window_mean)\n","    # features.append(window_std)\n","\n","    month=times[i].month\n","\n","    y_1=model.predict(np.array([np.array(features)]))[0]\n","\n","    pred_y.append(y_1)\n","    last_lag_y.pop(0)\n","    last_lag_y.append(y_1)\n","    \n","    #if(i<5): print(features[-10:])\n","\n","  for ii in range(len(pred_y)): pred_y[ii] *= y_mn\n","\n","  return pred_y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHolK5ToyGXI"},"source":["# pred_building_1 = calculatePredictions(1, start_index=-22000, number_of_days_to_predict=31)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YHa4QIP5lJ8"},"source":["starting_indexes = [-22000]*6 + [-40000]*6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6NiPHSx0-kL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633762826844,"user_tz":-330,"elapsed":82146,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"3c33bf80-ba46-4e5d-93e6-fbeb419edfce"},"source":["sub_arr = []\n","sub_indexes = []\n","\n","for i in range(0,6):\n","  sub_indexes.append(loaded_data.iloc[i]['series_name'])\n","  # x = np.random.rand(2880)\n","  \n","  # sub_arr.append(x*solar_mean_array[i])\n","\n","  pred_building_1 = calculatePredictions(1, start_index=starting_indexes[i], number_of_days_to_predict=31)\n","  \n","  sub_arr.append(pred_building_1)\n","\n","sub_arr = np.array(sub_arr)\n","sub_arr.shape"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in true_divide\n"]},{"output_type":"stream","name":"stdout","text":["Training until validation scores don't improve for 100 rounds.\n","Did not meet early stopping. Best iteration is:\n","[2]\tvalid_0's mape: 0.2943\n"]},{"output_type":"execute_result","data":{"text/plain":["(1, 2976)"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"W4V9wMlX1edq"},"source":["df_sub = pd.DataFrame(data=sub_arr, index=sub_indexes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnE1mX6-1id8","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1633762826857,"user_tz":-330,"elapsed":50,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"4ae04493-b598-4dbe-89dc-cbba99457c20"},"source":["df_sub.head(15)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>2936</th>\n","      <th>2937</th>\n","      <th>2938</th>\n","      <th>2939</th>\n","      <th>2940</th>\n","      <th>2941</th>\n","      <th>2942</th>\n","      <th>2943</th>\n","      <th>2944</th>\n","      <th>2945</th>\n","      <th>2946</th>\n","      <th>2947</th>\n","      <th>2948</th>\n","      <th>2949</th>\n","      <th>2950</th>\n","      <th>2951</th>\n","      <th>2952</th>\n","      <th>2953</th>\n","      <th>2954</th>\n","      <th>2955</th>\n","      <th>2956</th>\n","      <th>2957</th>\n","      <th>2958</th>\n","      <th>2959</th>\n","      <th>2960</th>\n","      <th>2961</th>\n","      <th>2962</th>\n","      <th>2963</th>\n","      <th>2964</th>\n","      <th>2965</th>\n","      <th>2966</th>\n","      <th>2967</th>\n","      <th>2968</th>\n","      <th>2969</th>\n","      <th>2970</th>\n","      <th>2971</th>\n","      <th>2972</th>\n","      <th>2973</th>\n","      <th>2974</th>\n","      <th>2975</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Solar0</th>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.3795</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.4335</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.4335</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.4335</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.4335</td>\n","      <td>10.1685</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.381</td>\n","      <td>10.1685</td>\n","      <td>...</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","      <td>10.1685</td>\n","      <td>8.700562</td>\n","      <td>9.756</td>\n","      <td>9.756</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows Ã— 2976 columns</p>\n","</div>"],"text/plain":["           0         1       2        3     ...     2972      2973   2974   2975\n","Solar0  10.1685  8.700562  9.3795  10.1685  ...  10.1685  8.700562  9.756  9.756\n","\n","[1 rows x 2976 columns]"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"RAu5oeUb1uuR"},"source":["df_sub.to_csv(\"sub.csv\",header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBDx5mSw2-yv"},"source":["weather_df = pd.read_csv(\"ERA5_Weather_Data_Monash.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"ZRnF1fSD2_n5","executionInfo":{"status":"ok","timestamp":1633762940351,"user_tz":-330,"elapsed":380,"user":{"displayName":"Damika Gamlath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15397721094904153164"}},"outputId":"8618e526-299a-4386-ddef-19d91bcd4a4e"},"source":["weather_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datetime (UTC)</th>\n","      <th>coordinates (lat,lon)</th>\n","      <th>model (name)</th>\n","      <th>model elevation (surface)</th>\n","      <th>utc_offset (hrs)</th>\n","      <th>temperature (degC)</th>\n","      <th>dewpoint_temperature (degC)</th>\n","      <th>wind_speed (m/s)</th>\n","      <th>mean_sea_level_pressure (Pa)</th>\n","      <th>relative_humidity ((0-1))</th>\n","      <th>surface_solar_radiation (W/m^2)</th>\n","      <th>surface_thermal_radiation (W/m^2)</th>\n","      <th>total_cloud_cover (0-1)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2010-01-01 00:00:00</td>\n","      <td>(-37.91, 145.13)</td>\n","      <td>era5</td>\n","      <td>69.59</td>\n","      <td>10.0</td>\n","      <td>18.26</td>\n","      <td>16.39</td>\n","      <td>2.60</td>\n","      <td>101046.38</td>\n","      <td>0.89</td>\n","      <td>287.01</td>\n","      <td>408.35</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2010-01-01 01:00:00</td>\n","      <td>(-37.91, 145.13)</td>\n","      <td>era5</td>\n","      <td>69.59</td>\n","      <td>10.0</td>\n","      <td>18.67</td>\n","      <td>16.29</td>\n","      <td>2.91</td>\n","      <td>101037.96</td>\n","      <td>0.86</td>\n","      <td>360.79</td>\n","      <td>411.02</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2010-01-01 02:00:00</td>\n","      <td>(-37.91, 145.13)</td>\n","      <td>era5</td>\n","      <td>69.59</td>\n","      <td>10.0</td>\n","      <td>18.16</td>\n","      <td>15.89</td>\n","      <td>3.26</td>\n","      <td>101017.26</td>\n","      <td>0.87</td>\n","      <td>291.54</td>\n","      <td>410.67</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2010-01-01 03:00:00</td>\n","      <td>(-37.91, 145.13)</td>\n","      <td>era5</td>\n","      <td>69.59</td>\n","      <td>10.0</td>\n","      <td>18.46</td>\n","      <td>15.33</td>\n","      <td>3.17</td>\n","      <td>101022.56</td>\n","      <td>0.82</td>\n","      <td>357.11</td>\n","      <td>410.95</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2010-01-01 04:00:00</td>\n","      <td>(-37.91, 145.13)</td>\n","      <td>era5</td>\n","      <td>69.59</td>\n","      <td>10.0</td>\n","      <td>18.53</td>\n","      <td>15.11</td>\n","      <td>2.95</td>\n","      <td>100940.03</td>\n","      <td>0.80</td>\n","      <td>459.91</td>\n","      <td>410.00</td>\n","      <td>0.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        datetime (UTC)  ... total_cloud_cover (0-1)\n","0  2010-01-01 00:00:00  ...                     1.0\n","1  2010-01-01 01:00:00  ...                     1.0\n","2  2010-01-01 02:00:00  ...                     1.0\n","3  2010-01-01 03:00:00  ...                     1.0\n","4  2010-01-01 04:00:00  ...                     0.9\n","\n","[5 rows x 13 columns]"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","metadata":{"id":"lpZip-l23BBX"},"source":[""],"execution_count":null,"outputs":[]}]}